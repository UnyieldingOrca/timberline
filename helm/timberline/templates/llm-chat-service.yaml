{{- if and .Values.llmChat.enabled (not .Values.llmChat.external.enabled) }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-chat
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "timberline.llmChat.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.llmChat.replicas }}
  selector:
    matchLabels:
      app: llama-cpp-chat
  template:
    metadata:
      labels:
        app: llama-cpp-chat
        {{- include "timberline.selectorLabels" . | nindent 8 }}
    spec:
      {{- if .Values.llmChat.model.download }}
      initContainers:
      - name: download-model
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          if [ ! -f /models/{{ .Values.llmChat.model.name }} ]; then
            echo "Downloading chat model..."
            wget -O /models/{{ .Values.llmChat.model.name }} \
              {{ .Values.llmChat.model.url }}
          else
            echo "Model already exists"
          fi
        volumeMounts:
        - name: models
          mountPath: /models
      {{- end }}
      containers:
      - name: llama-cpp
        image: {{ .Values.llmChat.image.repository }}:{{ .Values.llmChat.image.tag }}
        imagePullPolicy: {{ .Values.llmChat.image.pullPolicy }}
        command:
        - /app/llama-server
        args:
        - -m
        - /models/{{ .Values.llmChat.model.name }}
        - --host
        - "0.0.0.0"
        - --port
        - {{ .Values.llmChat.config.port | quote }}
        - --threads
        - {{ .Values.llmChat.config.threads | quote }}
        - --ctx-size
        - {{ .Values.llmChat.config.ctxSize | quote }}
        - --temp
        - {{ .Values.llmChat.config.temperature | quote }}
        - --repeat-penalty
        - {{ .Values.llmChat.config.repeatPenalty | quote }}
        ports:
        - containerPort: {{ .Values.llmChat.config.port }}
          name: http
        volumeMounts:
        - name: models
          mountPath: /models
        livenessProbe:
          httpGet:
            path: /health
            port: {{ .Values.llmChat.config.port }}
          initialDelaySeconds: 120
          periodSeconds: 60
        readinessProbe:
          httpGet:
            path: /health
            port: {{ .Values.llmChat.config.port }}
          initialDelaySeconds: 60
          periodSeconds: 30
        resources:
          {{- toYaml .Values.llmChat.resources | nindent 10 }}
      volumes:
      - name: models
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: llama-cpp-chat
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "timberline.llmChat.labels" . | nindent 4 }}
spec:
  type: {{ .Values.llmChat.service.type }}
  ports:
  - port: {{ .Values.llmChat.service.port }}
    targetPort: {{ .Values.llmChat.config.port }}
    name: http
    {{- if eq .Values.llmChat.service.type "NodePort" }}
    nodePort: {{ .Values.llmChat.service.nodePort }}
    {{- end }}
  selector:
    app: llama-cpp-chat
{{- end }}
